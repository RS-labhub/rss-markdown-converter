I recently sat down with Anton Arhipov from JetBrains to talk about IntelliJ IDEA's shift toward becoming an AI-powered IDE. 

He walked me through some of the intelligent plugins, like context-aware code suggestions and machine learning–driven refactoring, that are fundamentally improving code accuracy and accelerating development cycles. 

If you're curious where IDEs and developer tools are headed, links to the full episode are in the comments.

---

• What is your favorite AI Code Generation Tool? Bolt.new? Base44? Cursor?
• What development skills are more critical now than ever before?
• Do you trust agentic AI to process large-scale automation for you?

The State of AI Native Dev survey is NOW OPEN! If you could do me a solid and spend 10-15 mins filling out your experiences with AI Dev Tooling, or share this post/link far and wide, I would forever appreciate you!

Your voice matters!

---

One of my ALL TIME favourite tech games is Galdalf, where you have to trick the LLM into giving you it's password before moving on to the next level. It's fun to play but really highlights one of the biggest issues facing us in LLMs: 

How can the LLM reason about or decide upon whether or not it's action is secure or insecure, right or wrong? Mateo Rojas-Carulla Co-founder and Chief Scientist at Lakera talks to Guy Podjarny from Tessl on the AI Native Dev. Be sure to watch the episode below:

---

Many companies failed to adopt DevOps because they didn't prepare for change fast enough in their team culture. We NEED to learn from those mistakes in the era of AI. Here are my learnings in chatting with Hannah Foxwell:

• Bottom-Up and Top-Down Adoption: AI adoption mirrors DevOps in its grassroots enthusiasm among engineers but contrasts with a much stronger top-down demand from leadership eager for productivity gains - are dev leads feeling AI adoption pressures from their management?

• Learning Curve for Mid-Level Developers: Experienced engineers can leverage AI effectively due to their expertise, while junior engineers grow up as "AI natives." However, mid-level engineers may face challenges adapting and require targeted mentorship and training.

• Rethinking Prioritization: With AI accelerating the build process, organizations must invest in better prioritization as they will be able to deliver much more, faster. How can we avoid feature bloat and ensure meaningful product improvements?

• The Evolution of Bottlenecks: Today the bottleneck is building applications - If AI shifts the bottleneck from code creation to other areas like product management, hypothesis testing, and prioritization. Teams must now focus on refining build-measure-learn cycles to ensure they can build effectively.

• AI's Dual Impact: Unlike DevOps, which was developer-centric, AI touches every role across an organization. This expands the reach, creating both opportunities and challenges, necessitating cross-departmental strategies and guidelines.

Many more great points made, and I really enjoyed the discussion! Thanks, Hannah Foxwell.

Please share and subscribe for future episodes!

---

First episode of the new year just landed on the AI Native Dev! I really enjoyed chatting with Macey Baker about her learnings and tips around the best way of constructing LLM prompts. Some of the gems we talked about include:

• Use Task Framing rather than ordering constraints in your prompts. This avoids LLMs ignoring your requests
• Give examples to your LLM in the prompts - tell it what good and bad responses look like in tagged sections
• Structure your inputs in digestible ways. Consider XML like tagging to achieve this 
• Prompts are disposable, specs aren't, but we can use LLMs to help us get better specs
• Invoke a thought process and throw away the answer - At Tessl, Macey uses the say() mechanism to prime an LLM before asking requests that require a response.

This session was hugely practical and valuable to anyone who could benefit from structuring better prompts.

#NativeAIDev #LLMPrompts #AIEngineering #PromptEngineering #MaceyBaker #Tessl

---

A few days ago, an LLM deleted Jason M. Lemkin's production database.

Why did this happen, and what should we do about it? 

Jason chronicles his experience vibe-coding his way to a (working?) web app using Replit's coding agent. 

A few days ago, it deleted his production database. 

The issue, of course, isn't the engineers working on Replit. They are a brilliant group and they built a pretty great product.

The real issue is LLMs - or more specifically - the current state of LLMs. Did you know that the state of the art in instruction-following benchmarks (like MultiChallenge by Scale AI) is currently 63% accuracy? 

In other words, given a long and complex conversation (like vibe coding sessions), there is a 37%(!!) chance that the best LLM in the world will ignore some instructions.

Sometimes that instruction is "don't delete production databases". 

How do we fix this? 

LLMs are great at many things, but they need guardrails. Deterministic code that MUST run to verify that what they "want" to do is safe. In some cases, the stakes are low, and LLMs are practically supervised (a human is watching them work), in others, a mistake is catastrophic. 

LLMs use tools to interact with the world. These tools need to be governed by policies. If an LLM tries to do something that's forbidden, it cannot be able to do that. 

This is why we build Atlas by Ariga. Originally, to prevent developers from shooting themselves in the foot when it comes to making changes to the database, but the exact same engine is relevant for supervising agents. 

We literally have a policy called "Prevent destructive changes". 

The next phase for LLMs isn't larger context windows or cheaper inference - it's back to the fundamentals that make software usable: determinism and safety.

---
